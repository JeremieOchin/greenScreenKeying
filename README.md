# Green Screen Keying
Some tests using CNN to generate an alpha channel from images captured with a green screen, for a use in VFX / compositing of images

## What is compositing ?
Compositing is a part of the post production process in the film industry where you assemble various images into one. The sources of these images are mixed : some comes from "real footage" (captured by a camera), some others are generated by computers, using 3D techniques (using 3D softwares) or 2D techniques (using 2D softs like Photoshop). 

## What is Keying ?
Keying is the term usually used by the compositors to extract the foreground of images shot on a greenscreen, removing the green screen and replacing it by something else. They can do it by using specific softwares that can isolate the green color of the screen and make it "transparent" in order to replace it by other images.

## What's the point of using a convolutionnal neural net for Keying ?
Keying can be a tedious process for various reasons : first off, during the shooting, the greenscreen may not be lit correctly, or there may be problem of exposures during capture, making it difficult to isolate the green screen. On top of that, you can have "motion blur", or semi-transparent areas, or thin objects like hairs that are difficult to treat, compared to hard edges where the limit between the object and the screen is more obvious.
So compositors usually use various "keyers" for a same shot, and then have to manually "mix" those areas of transparencies. They work on the image "per zones".

==> Could a CNN perform faster than a human on keying if we provide him good examples ? COuld it reach a sufficient level of quality in this process ?

## How does it work ?
Images are given with a PNG format, easily readable by OpenCV (and probably Tensorflow, but I used OpenCV for knowing it better). Those images contain 3 channels (Red, Green and Blue), coded on 8 bit. I later convert them to float by dividing by 255, to get a value between 0 and 1.

I have used footage from the online training company FXPHD, because I could get those PNG but also have the "alpha channel", which is a 4th channel that contain the degree of transparency of a given pixel (same as above, I convert it to float to get a value between 0 and 1).

### First attempt : regression

I have setup a basic Convolutionnal Neural Net to learn how to predict the transparency of each pixel :

- Inputs : I read batches of 4 images Full HD (1920 x 1080 x 3 channels). I split them in 9 (3 x 3) using OpenCV, for memory purposes. I then recombine the output.

- first layer : convolution with a kernel size of 5 x 5 pixels, a stride of 1 x 1 (same resolution) and activation ReLu. That layer gives me 32 features map. So the output tensor is of shape (batch, img_size_W, img_size_H, 32)

- second layer : I do a maxpooling on the depth (channels), with a stride of 2. That gives me a tensor of shape (batch, img_size_W, img_size_H, 16)

- third layer : convolution layer of kernel size 1x1, stride 1x1 that produces 32 feature maps. Activation ReLu

- fourth layer : convolution layer of kernel size 3x3, stride 1x1 that produces 16 feature maps. Activation ReLu as well.

- fifth layer : convolution layer of kernel size 1x1 with output 1 feature map, and activation Sigmoid to get the transparency between 0 and 1.

The training has last 2 hours for 100 epochs.

I already shared the trained network if you want to test it.

ENVIRONMENT :  
Windows 7  
Python 3.6.8  
Numpy 1.15.4  
OpenCV 3.4.1  
Tensorflow(-gpu) 1.12.0  

**Results** :

Here is a sample of the training set :

**Input image : **

![RGB Image from the training set](https://www.dropbox.com/s/ngawy2oygqy4rjx/Data.01.RGB.0063.png?raw=1)

**Alpha from training set : **

![Alpha from training set](https://www.dropbox.com/s/2xz5owz8x8betgc/Data.01.ALPHA.0063.png?raw=1)

**Alpha obtained after training : **

![Resulting alpha](https://www.dropbox.com/s/93rjwcmv3pyelas/reconstructedAlpha.png?raw=1)

**Learned kernels of the first layer : **

![kernels first layer](https://www.dropbox.com/s/kr83gbaik4o3j7l/kernel_gsc_01.png?raw=1)

**Test on a sequence the Network has not seen before to test generalization : **

![RGB Test for Generalization](https://www.dropbox.com/s/kbl1amfo0intqbi/testFootage.0001.png?raw=1)

![Resulting alpha from network](https://www.dropbox.com/s/avu3e7p51wsn6pv/resTestGPUregv2.png?raw=1)

### Change of approach : classification problem

The results shows that the CNN has a hard time figuring out the parts of the alpha that should be pitch black or totally white, leaving the intermediary values only for smoothly defined hedges (like hairs and whatnot).

My alpha being coded on 8 bits (256 values), maybe the CNN can learn to "classify" each pixel into 256 "classes of transaprency" ?

I have therefore created another network for that purposes : it outputs for each pixel 256 probabilities of belonging to one of those classes. It should improve the ability of the network to output the correct pure black / pure white classes, although it may be more complex to get the right intermediary values...

Now the CNN has the following structure, with two streams from the input :

- Inputs : I read batches of 8 elements of images Full HD (1920 x 1080 x 3 channels). Each element is 1/16 of the full HD initial image, in order to save memory (the network is trained on a Quadro K6000 that has 12 Gb of memory).

Stream 1 :

- first layer : convolution with a kernel size of 7 x 7 pixels, a stride of 1 x 1 (same resolution) and activation ReLu. That layer gives me 64 features map.

- second layer : maxpooling, with a stride of 2 and size of 3.

- third layer : local response normalization with depth 5

- fourth layer : convolution layer of kernel size 1x1, stride 1x1 that produces 128 feature maps. Activation ReLu

- fifth layer : convolution layer of kernel size 3x3, stride 2x2 that produces 128 feature maps. Activation ReLu.

- sixth layer : local response normalization with depth 5

- seventh layer : maxpooling, with a stride of 2 and size of 3.

- eigth layer : resize of the tensor to the original size (1/16 of Full HD format)

- nineth layer : convolution layer of kernel size 5x5, stride 1x1 that produces 64 feature maps. Activation ReLu as well.

Stream 2 :

- unique layer : convolution layer of kernel size 1x1, stride 1x1 that produces 128 feature maps. Activation ReLu.

Merging of the 2 streams with a concatenation of both tensors (that have the same size)

Then a final convolution layer of kernel size 1x1, stride 1x1 that produces 256 feature maps. Activation ReLu.

The data for the training is made creating a OneHot tensor from the alpha channel of the data set : it is therefore of dimension (1/16 of 1920 x 1080) x 256 : each pixel of the alpha channel is used to encode a vector of length 256, "filled" with 0 everywhere except a "1" on the i-th coordinate when the value of the pixel is "i" (with 0 <= i <= 255). The alpha is coded on 8 bit.

Then the objective function is evaluated using a softmax cross entropy layer. NOTE : I may have to change the ReLu activation on my last convolution layer, to get proper "logits".

